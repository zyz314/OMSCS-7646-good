{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = [\n",
    "    [0.885, 0.33,  9.1, 4],\n",
    "    [0.725, 0.39, 10.9, 5],\n",
    "    [0.560, 0.50,  9.4, 6],\n",
    "    [0.735, 0.57,  9.8, 5],\n",
    "    [0.610, 0.63,  8.4, 3],\n",
    "    [0.260, 0.63, 11.8, 8],\n",
    "    [0.500, 0.68, 10.5, 7],\n",
    "    [0.320, 0.78, 10.0, 6],\n",
    "]\n",
    "data = np.array(example_data)\n",
    "data_x, data_y = data[:,:-1], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0f8f830e2408>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# apply the function over the axis 0 of x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_over_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcorr_with_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# r is a 1x3x1 array, where each element is the correlation of a column of x with y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macal\\miniconda3\\envs\\ml4t\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_over_axes\u001b[1;34m(func, a, axes)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-0f8f830e2408>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# apply the function over the axis 0 of x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_over_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcorr_with_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# r is a 1x3x1 array, where each element is the correlation of a column of x with y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-0f8f830e2408>\u001b[0m in \u001b[0;36mcorr_with_y\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcorr_with_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# apply the function over the axis 0 of x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_over_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcorr_with_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macal\\miniconda3\\envs\\ml4t\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[1;34m(x, y, rowvar, bias, ddof)\u001b[0m\n\u001b[0;32m   2520\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[0;32m   2521\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[1;32m-> 2522\u001b[1;33m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2523\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2524\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macal\\miniconda3\\envs\\ml4t\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mcov\u001b[1;34m(m, y, rowvar, bias, ddof, fweights, aweights)\u001b[0m\n\u001b[0;32m   2384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrowvar\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2386\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mddof\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "def corr_with_y(a, axis):\n",
    "    return np.corrcoef(a, data_y)[0, 1]\n",
    "\n",
    "# apply the function over the axis 0 of x\n",
    "r = np.apply_over_axes(lambda a, axis: corr_with_y(a,axis), data_x, [0])\n",
    "\n",
    "# r is a 1x3x1 array, where each element is the correlation of a column of x with y\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09055067, 0.09724103, 1.6605908 ],\n",
       "       [0.19473596, 0.23044814, 3.50636422],\n",
       "       [0.16117655, 0.39074927, 4.88453195]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([\n",
    "    [np.random.rand(), np.random.random(), 21*np.random.random()],\n",
    "    [np.random.rand(), np.random.random(), 21*np.random.random()],\n",
    "    [np.random.rand(), np.random.random(), 21*np.random.random()],\n",
    "])\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = feature_column_number\n",
    "# 1= SplitVal\n",
    "# 2 = left_tree_start_rel\n",
    "# 3 = right_tree_start_rel]\n",
    "points = np.array([\n",
    "    [0.610, 0.63,  8.4, 3],\n",
    "    [0.500, 0.68, 10.5, 7],\n",
    "    [0.735, 0.57,  9.8, 5],\n",
    "    [0.260, 0.63, 11.8, 8],\n",
    "    [0.320, 0.78, 10.0, 6],\n",
    "    [0.885, 0.33,  9.1, 4],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the secret clue is 'zzyzx'\n"
     ]
    }
   ],
   "source": [
    "class LinRegLearner(object):  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    This is a Linear Regression Learner. It is implemented correctly.  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    :param verbose: If “verbose” is True, your code can print out information for debugging.  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        If verbose = False your code should not generate ANY output. When we test your code, verbose will be False.  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    :type verbose: bool  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    def __init__(self, verbose=False):  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        Constructor method  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        pass  # move along, these aren't the drones you're looking for  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    def author(self):  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :return: The GT username of the student  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :rtype: str  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        return \"mshihab6\"  # replace tb34 with your Georgia Tech username  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    def add_evidence(self, data_x, data_y):  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        Add training data to learner  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :param data_x: A set of feature values used to train the learner  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :type data_x: numpy.ndarray  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :param data_y: The value we are attempting to predict given the X data  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :type data_y: numpy.ndarray  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        # slap on 1s column so linear regression finds a constant term  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        new_data_x = np.ones([data_x.shape[0], data_x.shape[1] + 1])  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        new_data_x[:, 0 : data_x.shape[1]] = data_x  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        # build and save the model  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        self.model_coefs, residuals, rank, s = np.linalg.lstsq(  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "            new_data_x, data_y, rcond=None  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        )  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    def query(self, points):  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        Estimate a set of test points given the model we built.  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :param points: A numpy array with each row corresponding to a specific query.  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :type points: numpy.ndarray  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :return: The predicted result of the input data according to the trained model  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        :rtype: numpy.ndarray  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        \"\"\"  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        return (self.model_coefs[:-1] * points).sum(axis=1) + self.model_coefs[  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "            -1  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "        ]  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(f\"LinearRegression()\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(f\"LinearRegression()\")\n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "if __name__ == \"__main__\":  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "    print(\"the secret clue is 'zzyzx'\")  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Algorithm**\n",
    "```\n",
    "def build_tree(data):\n",
    "    if data.shape[0]==1: return [leaf,data.y,np.nan,np.nan]\n",
    "    if all data.y same: return [leaf,data.y,np.nan,np.nan]\n",
    "    else\n",
    "        determine best feature i to split on\n",
    "        SplitVal = data[:,i].median()\n",
    "        lefttree=build_tree(data[data[:,i]]<=SplitVal)\n",
    "        righttree=build_tree(data[data[:,i]]>SplitVal)\n",
    "        root = [i,SplitVal,1,lefttree.shape[0]+1]\n",
    "        return (append(root,lefttree,righttree))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTLearner(object):\n",
    "\n",
    "    # [x] __init__\n",
    "    def __init__(self,leaf_size=1, verbose=False):\n",
    "        \"\"\"\n",
    "        This is a Decision Tree Learner (DTLearner). You will need to properly implement this class as necessary.\n",
    "\n",
    "        Parameters\n",
    "            leaf_size (int)  - Is the maximum number of samples to be aggregated at a leaf\n",
    "            verbose (bool)   - If “verbose” is True, your code can print out information for debugging.\n",
    "                               If verbose = False your code should not generate ANY output. \n",
    "                               When we test your code, verbose will be False.\n",
    "        \"\"\"\n",
    "        self.leaf_size=leaf_size\n",
    "        self.verbose=verbose\n",
    "        self.learner = None # Will be used when building and querying\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(f\"DTLearner(leaf_size={self.leaf_size}, verbose={self.verbose})\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(f\"DTLearner(leaf_size={self.leaf_size}, verbose={self.verbose})\")\n",
    "    \n",
    "    # [x] add_evidence\n",
    "    def add_evidence(self, data_x, data_y):\n",
    "        \"\"\"\n",
    "            Add training data to learner\n",
    "\n",
    "            Parameters\n",
    "                data_x (numpy.ndarray) - A set of feature values used to train the learner\n",
    "                data_y (numpy.ndarray) - The value we are attempting to predict given the X data\n",
    "        \"\"\"\n",
    "        # data_x & data_y since the API requirements ask for it like that\n",
    "        self.learner = self.build_tree(data_x,data_y).reshape([-1,4])\n",
    "    \n",
    "    # Note: Seperated from main buid_tree code to accomodate for inheritance of Random Tree Learner\n",
    "    # [x] find_best_feature\n",
    "    def find_best_feature(self, data_x, data_y):\n",
    "        # Step 1: Correlation Matrix\n",
    "        correlation_matrix = np.corrcoef(data_x,data_y,rowvar=False)\n",
    "        # np.corrcoef to find the correlations between data_x and data_y\n",
    "        # rowvar set to false since data_x & data_y don't have the same shapes\n",
    "        # rowvar=False returns a correlation matrix where each column is compared with every column\n",
    "        \n",
    "        # Step 2: Extract the correlation array for our label column\n",
    "        correlation_y = correlation_matrix[:-1,-1]\n",
    "        # Since we only care about the correlation values with data_y, we only want the last column\n",
    "        # The last column ends in a 1 since data_y will have a correlation of 1 with itself, so remove it\n",
    "        \n",
    "        # Step 3: Find the column that has the LARGEST correlation with label\n",
    "        i = np.argmax(np.abs(correlation_y)) # This approach will automatically choose the first value if all are equal\n",
    "        # argmax finds the maximum value and returns the position of it, which is what we care about in the end\n",
    "        # Note, we care about finding the largest value in general, not the largest positive value, so we need to take the absolute value\n",
    "        return i\n",
    "    \n",
    "    # Note: Seperated from main buid_tree code to accomodate for inheritance of Random Tree Learner\n",
    "    # [x] find_SplitVal\n",
    "    def find_SplitVal(self, data_x, i):\n",
    "        SplitVal = np.median(data_x[:,i])\n",
    "        return SplitVal\n",
    "    \n",
    "    # [x] build_tree\n",
    "    def build_tree(self, data_x, data_y):\n",
    "        \"\"\"\n",
    "            Building learner with given data using recurrsion\n",
    "\n",
    "            Parameters\n",
    "                data_x (numpy.ndarray) - A set of feature values used to train the learner\n",
    "                data_y (numpy.ndarray) - The value we are attempting to predict given the X data\n",
    "                \n",
    "            Returns\n",
    "                The numpy array that represents our tree\n",
    "\n",
    "            Return type\n",
    "                np.ndarray\n",
    "        \"\"\"\n",
    "        # data_x & data_y since the API requirements ask for it like that\n",
    "        \n",
    "        # if there leaf_size number of rows, return\n",
    "        if data_x.shape[0] <= self.leaf_size: \n",
    "            return np.array([-1, np.mean(data_y), np.nan, np.nan])\n",
    "        \n",
    "        # if there is only one value in y (example: y=[1,1,1,1,1,1]), return\n",
    "        if len(np.unique(data_y)) == 1: return np.array([-1, data_y[0], np.nan, np.nan])\n",
    "        \n",
    "        # else: determine best feature i to split on\n",
    "        i = self.find_best_feature(data_x, data_y)\n",
    "        \n",
    "        # Find the Split Value [median]\n",
    "        SplitVal = self.find_SplitVal(data_x, i)\n",
    "        LeftSplitCond = data_x[:,i]<=SplitVal\n",
    "        # On the off chance that every condition in LeftSplitCond is all true or all false, return\n",
    "        if (np.all((LeftSplitCond == False)) | np.all((LeftSplitCond == True))):\n",
    "            return np.array([-1, np.mean(data_y), np.nan, np.nan])\n",
    "        RightSplitCond = data_x[:,i]>SplitVal\n",
    "        \n",
    "        # Build Left Side of Tree\n",
    "        leftdata_x,leftdata_y = data_x[LeftSplitCond], data_y[LeftSplitCond]\n",
    "        lefttree = self.build_tree(leftdata_x,leftdata_y)\n",
    "        \n",
    "        # Build Right Side of Tree\n",
    "        rightdata_x,rightdata_y = data_x[RightSplitCond], data_y[RightSplitCond]\n",
    "        righttree = self.build_tree(rightdata_x,rightdata_y) \n",
    "        \n",
    "        # Return Full Tree\n",
    "        root = np.array([i, SplitVal, 1, lefttree.reshape([-1,4]).shape[0]+1]) \n",
    "        # Note: np.append was not working as intended. Used np.concatenate\n",
    "        tree = np.concatenate([root,lefttree,righttree])\n",
    "        return tree\n",
    "\n",
    "    # [x] author\n",
    "    def author(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            The GT username of the student\n",
    "\n",
    "        Return type\n",
    "            str\n",
    "        \"\"\"\n",
    "        return \"mshihab6\"\n",
    "\n",
    "    # TODO Check Output With Test\n",
    "    # [x] query\n",
    "    def query(self, points):\n",
    "        \"\"\"\n",
    "        Estimate a set of test points given the model we built.\n",
    "\n",
    "        Parameters\n",
    "            points (numpy.ndarray) - A numpy array with each row corresponding to a specific query.\n",
    "\n",
    "        Returns\n",
    "            The predicted result of the input data according to the trained model\n",
    "\n",
    "        Return type\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "        ypred = np.array([])\n",
    "        for point in points:\n",
    "            i = 0\n",
    "            node = 0\n",
    "            while i >= 0:\n",
    "                i = int(self.learner[node][0])\n",
    "                if i < 0: break\n",
    "                SplitVal = self.learner[node][1]\n",
    "                SplitCon = point[i]<=SplitVal\n",
    "                if SplitCon:\n",
    "                    node += self.learner[node][2]\n",
    "                else:\n",
    "                    node += self.learner[node][3]\n",
    "                node = int(node)\n",
    "            ypred = np.append(ypred,self.learner[node][1])\n",
    "        return ypred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Tree Algorithm**\n",
    "```\n",
    "def build_tree(data):\n",
    "    if data.shape[0]==1: return [leaf,data.y,np.nan,np.nan]\n",
    "    if all data.y same: return [leaf,data.y,np.nan,np.nan]\n",
    "    else\n",
    "        determine random feature i to split on\n",
    "        SplitVal = (data[random,i]+data[random,i])/2\n",
    "        lefttree=build_tree(data[data[:,i]]<=SplitVal)\n",
    "        righttree=build_tree(data[data[:,i]]>SplitVal)\n",
    "        root = [i,SplitVal,1,lefttree.shape[0]+1]\n",
    "        return (append(root,lefttree,righttree))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTLearner(object):\n",
    "\n",
    "    # [x] __init__\n",
    "    def __init__(self,leaf_size=1, verbose=False):\n",
    "        \"\"\"\n",
    "        This is a Random Tree Learner (RTLearner). You will need to properly implement this class as necessary.\n",
    "\n",
    "        Parameters\n",
    "            leaf_size (int)  - Is the maximum number of samples to be aggregated at a leaf\n",
    "            verbose (bool)   - If “verbose” is True, your code can print out information for debugging.\n",
    "                               If verbose = False your code should not generate ANY output. \n",
    "                               When we test your code, verbose will be False.\n",
    "        \"\"\"\n",
    "        self.leaf_size=leaf_size\n",
    "        self.verbose=verbose\n",
    "        self.learner = None # Will be used when building and querying\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return(f\"RTLearner(leaf_size={self.leaf_size}, verbose={self.verbose})\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(f\"TTLearner(leaf_size={self.leaf_size}, verbose={self.verbose})\")\n",
    "    \n",
    "    # [x] add_evidence\n",
    "    def add_evidence(self, data_x, data_y):\n",
    "        \"\"\"\n",
    "            Add training data to learner\n",
    "\n",
    "            Parameters\n",
    "                data_x (numpy.ndarray) - A set of feature values used to train the learner\n",
    "                data_y (numpy.ndarray) - The value we are attempting to predict given the X data\n",
    "        \"\"\"\n",
    "        # data_x & data_y since the API requirements ask for it like that\n",
    "        self.learner = self.build_tree(data_x,data_y).reshape([-1,4])\n",
    "    \n",
    "    # [x] find_best_feature\n",
    "    def find_best_feature(self, data_x):\n",
    "        i = np.random.randint(0,data_x.shape[1])\n",
    "        return i\n",
    "\n",
    "    # [x] find_SplitVal\n",
    "    def find_SplitVal(self, data_x, i):\n",
    "        # random_row_1 = np.random.randint(0, data_x.shape[0])\n",
    "        # random_row_2 = np.random.randint(0, data_x.shape[0])\n",
    "        # while random_row_2 == random_row_1:\n",
    "        #     random_row_2 = np.random.randint(0, data_x.shape[0])\n",
    "        # SplitVal = (data_x[random_row_1,i]+data_x[random_row_2,i])/2\n",
    "        SplitVal = np.median(data_x[:,i])\n",
    "        return SplitVal\n",
    "    \n",
    "    # [x] build_tree\n",
    "    def build_tree(self, data_x, data_y):\n",
    "        \"\"\"\n",
    "            Building learner with given data using recurrsion\n",
    "\n",
    "            Parameters\n",
    "                data_x (numpy.ndarray) - A set of feature values used to train the learner\n",
    "                data_y (numpy.ndarray) - The value we are attempting to predict given the X data\n",
    "                \n",
    "            Returns\n",
    "                The numpy array that represents our tree\n",
    "\n",
    "            Return type\n",
    "                np.ndarray\n",
    "        \"\"\"\n",
    "        # data_x & data_y since the API requirements ask for it like that\n",
    "        \n",
    "        # if there leaf_size number of rows, return\n",
    "        if data_x.shape[0] <= self.leaf_size: \n",
    "            return np.array([-1, np.mean(data_y), np.nan, np.nan])\n",
    "        \n",
    "        # if there is only one value in y (example: y=[1,1,1,1,1,1]), return\n",
    "        if len(np.unique(data_y)) == 1: return np.array([-1, data_y[0], np.nan, np.nan])\n",
    "        \n",
    "        # else: determine best feature i to split on\n",
    "        i = self.find_best_feature(data_x)\n",
    "        \n",
    "        # Find the Split Value [median]\n",
    "        SplitVal = self.find_SplitVal(data_x, i)\n",
    "        LeftSplitCond = data_x[:,i]<=SplitVal\n",
    "        # On the off chance that every condition in LeftSplitCond is all true or all false, return\n",
    "        if (np.all((LeftSplitCond == False)) | np.all((LeftSplitCond == True))):\n",
    "            return np.array([-1, np.mean(data_y), np.nan, np.nan])\n",
    "        RightSplitCond = data_x[:,i]>SplitVal\n",
    "        \n",
    "        # Build Left Side of Tree\n",
    "        leftdata_x,leftdata_y = data_x[LeftSplitCond], data_y[LeftSplitCond]\n",
    "        lefttree = self.build_tree(leftdata_x,leftdata_y)\n",
    "        \n",
    "        # Build Right Side of Tree\n",
    "        rightdata_x,rightdata_y = data_x[RightSplitCond], data_y[RightSplitCond]\n",
    "        righttree = self.build_tree(rightdata_x,rightdata_y) \n",
    "        \n",
    "        # Return Full Tree\n",
    "        root = np.array([i, SplitVal, 1, lefttree.reshape([-1,4]).shape[0]+1]) \n",
    "        # Note: np.append was not working as intended. Used np.concatenate\n",
    "        tree = np.concatenate([root,lefttree,righttree])\n",
    "        return tree\n",
    "\n",
    "    # [x] author\n",
    "    def author(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            The GT username of the student\n",
    "\n",
    "        Return type\n",
    "            str\n",
    "        \"\"\"\n",
    "        return \"mshihab6\"\n",
    "\n",
    "    # TODO Check Output With Test\n",
    "    # [x] query\n",
    "    def query(self, points):\n",
    "        \"\"\"\n",
    "        Estimate a set of test points given the model we built.\n",
    "\n",
    "        Parameters\n",
    "            points (numpy.ndarray) - A numpy array with each row corresponding to a specific query.\n",
    "\n",
    "        Returns\n",
    "            The predicted result of the input data according to the trained model\n",
    "\n",
    "        Return type\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "        ypred = np.array([])\n",
    "        for point in points:\n",
    "            i = 0\n",
    "            node = 0\n",
    "            while i >= 0:\n",
    "                i = int(self.learner[node][0])\n",
    "                if i < 0: break\n",
    "                SplitVal = self.learner[node][1]\n",
    "                SplitCon = point[i]<=SplitVal\n",
    "                if SplitCon:\n",
    "                    node += self.learner[node][2]\n",
    "                else:\n",
    "                    node += self.learner[node][3]\n",
    "                node = int(node)\n",
    "            ypred = np.append(ypred,self.learner[node][1])\n",
    "        return ypred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BagLearner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagLearner(object):\n",
    "\n",
    "    def __init__(self,learner, kwargs = {}, bags = 20, boost = False, verbose = False):\n",
    "        \"\"\"\n",
    "        This is a Bootstrap Aggregataion Learner (BagLearner). You will need to properly implement this class as necessary.\n",
    "\n",
    "        Parameters\n",
    "            learner (learner) - Points to any arbitrary learner class that will be used in the BagLearner.\n",
    "            kwargs            - Keyword arguments that are passed on to the learner's constructor and they can vary according to the learner\n",
    "            bags (int)        - The number of learners you should train using Bootstrap Aggregation. \n",
    "                                If boost is true, then you should implement boosting (optional implementation).\n",
    "            verbose (bool)    - If “verbose” is True, your code can print out information for debugging.\n",
    "                                If verbose = False your code should not generate ANY output. When we test your code, verbose will be False.\n",
    "        \"\"\"\n",
    "        self.learner = learner(**kwargs) # Define the Learner using the keyword arguments passed\n",
    "        self.kwargs = kwargs # Keep track of keyword arguments\n",
    "        self.bags = bags # Store the bag count\n",
    "        self.boost = boost # Store the boolean boost value\n",
    "        self.verbose = verbose # Store the boolean verbose value\n",
    "        self.ensemble = np.array([self.learner for _ in range(self.bags)]) # create numpy array of learners to make an ensemble\n",
    "\n",
    "    def __repr__(self):\n",
    "        return(f\"BagLearner(learner = {self.learner}, bags = {self.bags}, boost = {self.boost}, verbose = {self.verbose})\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(f\"BagLearner(learner = {self.learner}, bags = {self.bags}, boost = {self.boost}, verbose = {self.verbose})\")\n",
    "    \n",
    "    def create_bags(self, m, data_x, data_y):\n",
    "        n = len(data_x)\n",
    "        bag_indexes = np.random.choice(n,size=(m,n),replace=True) # creates a numpy array of shape (m,n) of row numbers per bag\n",
    "        bags = np.array([data_x[indexes,:] for indexes in bag_indexes]) # using each row numbers to create bags of shape (m,n,c) where c is the column number in data_x\n",
    "        bag_ys = np.array([data_y[indexes] for indexes in bag_indexes]) # using each row numbers to create the ys for each bag of shape (m,n)\n",
    "        return bags,bag_ys # return the created bags and their respective ys\n",
    "    \n",
    "    def add_evidence(self, data_x, data_y):\n",
    "        \"\"\"\n",
    "        Add training data to learner\n",
    "\n",
    "        Parameters\n",
    "            data_x (numpy.ndarray) - A set of feature values used to train the learner\n",
    "            data_y (numpy.ndarray) - The value we are attempting to predict given the X data\n",
    "        \"\"\"\n",
    "        bags_array, bag_ys = self.create_bags(self.bags, data_x, data_y) # get all the bags and their respective ys\n",
    "        for i, learner in enumerate(self.ensemble):\n",
    "            # Gotta Love Enumerate!\n",
    "            learner.add_evidence(bags_array[i],bag_ys[i]) # build learner from the bags created above\n",
    "            # No need to make a build learner since each learner should have it already\n",
    "\n",
    "\n",
    "    def author(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            The GT username of the student\n",
    "\n",
    "        Return type\n",
    "            str\n",
    "        \"\"\"\n",
    "        return \"mshihab6\"\n",
    "\n",
    "\n",
    "    def query(self, points):\n",
    "        \"\"\"\n",
    "        Estimate a set of test points given the model we built.\n",
    "\n",
    "        Parameters\n",
    "            points (numpy.ndarray) - A numpy array with each row corresponding to a specific query.\n",
    "\n",
    "        Returns\n",
    "            The predicted result of the input data according to the trained model\n",
    "\n",
    "        Return type\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for learner in self.ensemble:\n",
    "            predictions.append(learner.query(points))\n",
    "        avg = np.mean(np.array(predictions),axis=0)\n",
    "        return np.mean(np.array(predictions),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**InsaneLearner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsaneLearner(object):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose=verbose\n",
    "        self.learner = np.array([BagLearner(LinRegLearner) for _ in range(20)])\n",
    "    def __repr__(self):return(f\"InsaneLearner(learner = {self.learner}, verbose = {self.verbose})\")\n",
    "    def __str__(self):return(f\"InsaneLearner(learner = {self.learner}, verbose = {self.verbose})\")\n",
    "    def add_evidence(self,data_x, data_y):\n",
    "        for learner in self.learner: learner.add_evidence(data_x,data_y)\n",
    "    def author(self): return \"mshihab6\"\n",
    "    def query(self, points):\n",
    "        predictions = []\n",
    "        for learner in self.learner: predictions.append(learner.query(points))\n",
    "        return np.mean(np.array(predictions),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data_x)\n",
    "m = 20\n",
    "\n",
    "bag_indexes = np.random.choice(n,size=(m,n),replace=True)\n",
    "bags = np.array([data_x[indexes,:] for indexes in bag_indexes])\n",
    "bag_ys = np.array([data_y[indexes] for indexes in bag_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [4.5 7.  4.5 5. ]\n",
      "1 [5. 6. 6. 5.]\n",
      "2 [5. 7. 8. 5.]\n",
      "3 [5. 7. 6. 5.]\n",
      "4 [4.75 6.   6.   4.75]\n",
      "[4.85 6.6  6.1  4.95]\n"
     ]
    }
   ],
   "source": [
    "learners = np.array([RTLearner() for _ in range(5)])\n",
    "for i, l in enumerate(learners):\n",
    "    l.add_evidence(bags[i],bag_ys[i])\n",
    "i = [3,6,2,1]\n",
    "predictions = []\n",
    "\n",
    "for idx, l in enumerate(learners):\n",
    "    lq = l.query(data_x[i,:])\n",
    "    print(idx, lq)\n",
    "    predictions.append(lq)\n",
    "avg = np.mean(np.array(predictions),axis=0)\n",
    "print(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = open(r\"C:\\Users\\macal\\OneDrive\\Documents\\OMSCS\\CS7647\\assess_learners\\Data\\Istanbul.csv\") \n",
    "data = np.array([list(map(str,s.strip().split(','))) for s in inf.readlines()])\n",
    "# data = np.array([list(map(float, s.strip().split(','))) for s in inf.readlines()])\n",
    "inf.close()\n",
    "\n",
    "if data[0][0]==\"date\":\n",
    "    data = data[1:,1:]\n",
    "\n",
    "# compute how much of the data is training and testing  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "train_rows = int(0.6 * data.shape[0])  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "test_rows = data.shape[0] - train_rows  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "                                                                            \n",
    "# separate out training and testing data  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "train_x = data[:train_rows, 0:-1].astype(float)  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "train_y = data[:train_rows, -1].astype(float)  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "test_x = data[train_rows:, 0:-1].astype(float)  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "test_y = data[train_rows:, -1].astype(float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 4, 0, 5, 2, 7, 8, 3, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(10,size = 10, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bags(data_x, data_y):\n",
    "        n = len(data_x)\n",
    "        bags = list()\n",
    "        bag_ys = list()\n",
    "        indexes = np.random.choice(n, size = n, replace=True) # creates a numpy array of shape (n) of row numbers per bag\n",
    "        bags = np.array(data_x[indexes,:]) # using each row numbers to create bags of shape (n,c) where c is the column number in data_x\n",
    "        bag_ys = np.array(data_y[indexes]) # using each row numbers to create the ys for each bag of shape (n)\n",
    "        return bags, bag_ys # return the created bags and their respective ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bags, test_bag_ys = create_bags(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.885,  0.33 ,  9.1  ],\n",
       "        [ 0.725,  0.39 , 10.9  ],\n",
       "        [ 0.56 ,  0.5  ,  9.4  ],\n",
       "        [ 0.735,  0.57 ,  9.8  ],\n",
       "        [ 0.61 ,  0.63 ,  8.4  ],\n",
       "        [ 0.26 ,  0.63 , 11.8  ],\n",
       "        [ 0.5  ,  0.68 , 10.5  ],\n",
       "        [ 0.32 ,  0.78 , 10.   ]]),\n",
       " array([[ 0.56,  0.5 ,  9.4 ],\n",
       "        [ 0.26,  0.63, 11.8 ],\n",
       "        [ 0.56,  0.5 ,  9.4 ],\n",
       "        [ 0.32,  0.78, 10.  ],\n",
       "        [ 0.26,  0.63, 11.8 ],\n",
       "        [ 0.32,  0.78, 10.  ],\n",
       "        [ 0.5 ,  0.68, 10.5 ],\n",
       "        [ 0.56,  0.5 ,  9.4 ]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x, test_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8., 6., 6., 8., 6., 7., 6.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bag_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([170, 173, 220, 218, 134,  53, 276, 209, 165, 158, 132, 298, 216,\n",
       "       150,  75,   6, 242,  83,  62, 131, 286, 309, 246, 235, 140, 146,\n",
       "        57,  32,  34,  33,  40, 300, 243, 128,  74, 177, 163,  16, 184,\n",
       "        57, 132, 182, 175,  45,  55, 150, 275, 147, 263, 175, 229,  26,\n",
       "       243, 217, 293, 169,  60, 263,   8, 146, 230, 308, 120, 310, 206,\n",
       "       151,  79, 164, 300, 102,  41, 234, 254, 148, 315, 115, 258, 154,\n",
       "       121, 159,  43, 202,  92, 263,  74, 156, 260,  83,  51,  97, 294,\n",
       "       161, 113, 181,  25, 147, 186, 136, 146, 274, 126, 320, 203,   3,\n",
       "       285, 289, 164, 274, 238, 255,  29,  83, 313,  29,  37, 220,  76,\n",
       "       228, 300, 258, 253, 160, 190,  82,  66, 294, 313, 257,  36, 129,\n",
       "        31,  28,  67,  15, 180, 226, 277,  31, 139,  69, 194,  13, 172,\n",
       "        64, 289,  28, 271,  23, 222,   1,   5,  92, 194, 185,  27, 238,\n",
       "       172, 161,  48, 308, 282, 204,  11,  71, 228, 186, 132,  41, 287,\n",
       "       170, 260,  54, 282,  89, 318, 150, 162,  23, 266, 141, 151, 266,\n",
       "       292, 305, 175,  84,  21,  60,  26, 235, 319,  40,   3, 315, 207,\n",
       "       316, 248, 252, 167,   4,  52, 273, 168, 184,  46, 265,  71, 287,\n",
       "       263,  61, 210, 167, 287, 132,  84,  36, 308, 252,  22, 293, 165,\n",
       "       319,  16,  26, 172,  44, 313,  66,  70, 178, 220, 158,  81, 105,\n",
       "        78, 288, 147, 209, 123,  29, 149,  18, 146,  20,  99, 191, 232,\n",
       "       192, 320, 289, 298,  45, 132, 196, 299, 104, 226, 110, 275, 117,\n",
       "       237, 195, 250, 104, 235, 273, 174, 136,  28, 251,  24, 265, 283,\n",
       "       309,  29,  79,  13, 203, 188,  90, 172, 175,  14, 306, 290, 220,\n",
       "        11, 317,  21,  84,  88, 272, 297, 256, 217,  30, 180, 172, 289,\n",
       "       150, 204, 102, 193, 185,  41,  37,  79, 139, 218, 284,  89, 253,\n",
       "       185, 320,  60, 179, 313,  25,  20, 187, 112])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(1481090005)  \t\t  \t   \t\t \t   \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "bag_index = np.random.choice(range(train_x.shape[0]), train_x.shape[0], replace = True)\n",
    "bag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Bag Correlation: 0.6287403923747064 vs 20-Bag Correlation: 0.5527748137407896\n"
     ]
    }
   ],
   "source": [
    "bag_1  = BagLearner(learner=RTLearner,kwargs={\"leaf_size\": 1},bags=1 ,boost=False,verbose=False)\n",
    "bag_20 = BagLearner(learner=RTLearner,kwargs={\"leaf_size\": 1},bags=20,boost=False,verbose=False)\n",
    "bag_1 .add_evidence(train_x,train_y)\n",
    "bag_20.add_evidence(train_x,train_y)\n",
    "preds_1 = bag_1 .query(test_x)\n",
    "preds_20= bag_20.query(test_x)\n",
    "corr1  = np.corrcoef(preds_1 , test_y)[0, 1]\n",
    "corr20 = np.corrcoef(preds_20, test_y)[0, 1]\n",
    "print(f\"One-Bag Correlation: {corr1} vs 20-Bag Correlation: {corr20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSME(y_true, y_pred):\n",
    "    # rmse = math.sqrt(((y_true - y_pred) ** 2).sum() / y_true.shape[0])\n",
    "    rmse = math.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_true, y_pred):\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "    return mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ME(y_true, y_pred):\n",
    "    me = np.max(np.abs(y_true - y_pred))\n",
    "    return me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = sys.float_info.min\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    eps = sys.float_info.min\n",
    "    numerator = ((y_true-y_pred)**2).sum()\n",
    "    denominator = max(eps,((y_true-y_true.mean())**2).sum())\n",
    "    r2 = 1-(numerator/denominator)\n",
    "    return max(0,r2),r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
